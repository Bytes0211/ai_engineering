# Dev Notes

## Ollama

### ğŸ§© What Ollama Is  

Ollama is a **local LLM runtime** that lets you download, run, and manage large language models entirely on your machine. It provides:

- A lightweight **server** that runs models locally  
- A **CLI** for running, managing, and customizing models  
- A **Modelfile** system for building your own variants  
- An **HTTP API** for programmatic use  

Itâ€™s designed to make local LLMs as easy as running a Docker container.

### ğŸ› ï¸ Core Ollama CLI Commands  

The CLI is the primary interface for interacting with Ollama. Here are the essential commands, with citations.

#### â–¶ï¸ Run Models  

- `ollama run <model>` â€” Run a model interactively or with a prompt  
- Supports multiline input, images, embeddings, and JSON output

#### â¬‡ï¸ Download / Manage Models  

- `ollama pull <model>` â€” Download a model from the registry  
- `ollama ls` or `ollama list` â€” List installed models  
- `ollama rm <model>` â€” Remove a model  
- `ollama show <model>` â€” Display model details

#### ğŸ§± Create or Modify Models  

- `ollama create -f Modelfile` â€” Build a custom model from a Modelfile  
- `ollama cp <src> <dest>` â€” Copy a model

#### ğŸ–¥ï¸ Server & Process Management  

- `ollama serve` â€” Start the Ollama server (default port 11434)  
- `ollama ps` â€” List running models  
- `ollama stop <model>` â€” Stop a running model

#### ğŸ” Authentication  

- `ollama signin` / `ollama signout` â€” Manage cloud authentication

---

### ğŸ§­ Quick Summary Table

| Command | Purpose | Source |
|--------|---------|--------|
| `ollama run` | Run/chat with a model |  |
| `ollama pull` | Download a model |  |
| `ollama ls` / `list` | List installed models |  |
| `ollama rm` | Remove a model |  |
| `ollama create` | Build a model from a Modelfile |  |
| `ollama serve` | Start the Ollama server |  |
| `ollama ps` | List running models |  |
| `ollama stop` | Stop a running model |  |
| `ollama show` | Show model info |  |
| `ollama signin/signout` | Cloud auth |  |

## ğŸ’¬ Chat Completions API â€” What Is It 

The **Chat Completions API** is an endpoint used to generate model responses in a **chatâ€‘style format**, where the input is a list of messages and the output is a modelâ€‘generated message.  
OpenAI describes it as an API that â€œgenerates a model response from a list of messages comprising a conversationâ€.

Azureâ€™s documentation reinforces that chat models are **optimized for conversational interfaces**, expecting input in a structured chat transcript format and returning a modelâ€‘written message.

DeepWiki adds that the Chat Completions API is the **messageâ€‘based interface** for text generation and supports streaming, tool calling, and structured outputs.

---
For more information see [course notes](course_notes.md)

Here is the core coponents of the code. For mre deatil review [chatcomp.py](./my_code/chat_completion/chatcomp.py)

```py
load_dotenv(override=True)
api_key = os.getenv('OPENAI_API_KEY')

headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}

payload = {
    "model": "gpt-5-nano",
    "messages": [
        {"role": "user", "content": "Tell me a fun fact"}]
}

print("=" * 60)
print("ğŸš€ Sending request to OpenAI API...")
print("=" * 60)
print(f"\nğŸ“¦ Payload:")
print(json.dumps(payload, indent=2))
```

---

### ğŸ§© How It Works  

You send a request containing:

- A **model** (e.g., `gpt-4o`)
- A **messages array** (system/developer/user messages)
- Optional parameters (temperature, max tokens, top_p, etc.)

The API returns:

- A **completion** containing the modelâ€™s next message  
- Metadata (id, usage, finish_reason, etc.)

---
